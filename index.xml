<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Larry Dong</title>
    <link>https://www.larrydong.com/</link>
      <atom:link href="https://www.larrydong.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Larry Dong</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© 2021 Larry Dong</copyright><lastBuildDate>Sat, 17 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Larry Dong</title>
      <link>https://www.larrydong.com/</link>
    </image>
    
    <item>
      <title>üê¢ The slump...</title>
      <link>https://www.larrydong.com/gsoc2021/gsoc-part1/</link>
      <pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/gsoc-part1/</guid>
      <description>&lt;p&gt;With half of GSoC behind me (already!), progress in the past few weeks have been embarrassingly slow. The first challenge is learning how to learn. As I wrote in my previous post, there is so much that I don&amp;rsquo;t know; while this is inherently one of the exciting aspects of GSoC, it can be daunting to run around in circles. From the testing in the CD/CI pipeline to writing a likelihood for a sample from a Dirichlet Process, the first 5 weeks of GSoC have been invested into making mistakes. As my mentors have said in the first evaluation: &amp;ldquo;We encourage him to continue to focus on quantity rather than quality, and don&amp;rsquo;t be afraid to make mistakes &amp;ndash; a ton of learning occurs from iterating on code.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Although I have little code to actually talk about, here is a rundown of &amp;ldquo;mistakes&amp;rdquo; that I learned during the past weeks.&lt;/p&gt;
&lt;h2 id=&#34;mistake-1-attempting-to-get-too-involved-with-version-4-refactoring-of-pymc3&#34;&gt;Mistake 1: Attempting to get too involved with version 4 refactoring of PyMC3&lt;/h2&gt;
&lt;p&gt;PyMC3 is currently undergoing a refactoring process converting its backend structure to 
&lt;a href=&#34;https://github.com/aesara-devs/aesara&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aesara&lt;/a&gt; which, frankly, is difficult to wrap my head around. My attempt to refactor the $\chi^2$ distribution (see PR 
&lt;a href=&#34;https://github.com/aesara-devs/aesara/pull/414&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/pymc-devs/pymc3/pull/4695&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) was an &amp;ldquo;easy&amp;rdquo; exercise that took a long time to figure out during the (pre-)community bonding phase. Dirichlet processes are often used to model the latent parameter in mixture models such that the number of clusters need not to be specified. However, with &lt;code&gt;pm.Mixture&lt;/code&gt; and &lt;code&gt;pm.DensityDist&lt;/code&gt; needing refactoring, I initially wanted to challenge myself to help write an &lt;code&gt;rv_op&lt;/code&gt; for mixture models, but it turns out to be quite challenging (see PR 
&lt;a href=&#34;https://github.com/aesara-devs/aeppl/pull/19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;h4 id=&#34;solution-to-mistake-1&#34;&gt;&lt;em&gt;Solution to Mistake 1&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Rather than spend time figuring out how to rewrite a mixture &lt;code&gt;rv_op&lt;/code&gt;, it is more worthwhile to spend my time implementing a single class for Dirichlet processes (mixtures) and leverage &lt;code&gt;pm.Mixture&lt;/code&gt; once it has been refactored. Dirichlet processes are complicated on its own, so it is okay that I solely focus on them.&lt;/p&gt;
&lt;h2 id=&#34;mistake-2-coding-dirichlet-processes-without-a-likelihood&#34;&gt;Mistake 2: Coding Dirichlet processes without a likelihood&lt;/h2&gt;
&lt;p&gt;PyMC3 relies on using &lt;code&gt;pm.Model()&lt;/code&gt; as a context manager such that distributions can be posited on random variables and parameters. Given that a Dirichlet process $G$ can be rewritten as:&lt;/p&gt;
&lt;p&gt;\begin{align*}
G(x) = \sum_{h=1}^{\infty} w_h \delta_{m_h}(x)
\end{align*}&lt;/p&gt;
&lt;p&gt;for sticks $w_h$ and atoms $m_h$ (see below for more mathematical details), this is exactly what I coded using Aesara operations, which are very similar to NumPy ones.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc3 as pm
import numpy as np
import aesara.tensor as at

def stick_breaking(betas):
    sticks = at.concatenate([[1], at.cumprod(1 - betas[:-1])])

    return betas * sticks

Xs = np.array([-1, 0, 0.5, 1, 2])[..., np.newaxis]
K = 19 # number of sticks

with pm.Model() as model:
    betas = pm.Beta(&amp;quot;betas&amp;quot;, 1., np.ones(shape=(K,)))
    weights = pm.Deterministic(&amp;quot;weights&amp;quot;, stick_breaking(betas))
    G0 = pm.Normal(&amp;quot;base-dist&amp;quot;, 0, 1)

    dirac = at.lt(G0, Xs)
    linear_comb = at.sum(at.mul(weights, dirac), axis=1)

    prior_atoms = pm.Deterministic(
        name=&amp;quot;dp-prior&amp;quot;,
        var=linear_comb,
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked, but the issue is that &lt;code&gt;pm.Deterministic&lt;/code&gt; does not have a likelihood of its own. PyMC3 revolves around building a likelihood behind the scenes on which Hamiltonian Monte Carlo sampling methods can be executed. While the code above works, it is not the right direction when it comes to building an API and integrating it in more complex and potentially hierarchical models.&lt;/p&gt;
&lt;h4 id=&#34;solution-to-mistake-2&#34;&gt;&lt;em&gt;Solution to Mistake 2&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;The solution to this is &amp;ldquo;simply&amp;rdquo; to build a class that implements a likelihood for a Dirichlet process. However, how do we write a likelihood for a sample from a Dirichlet process? Do nonparametric methods, frequentist or Bayesian, have likelihoods? (The answer is yes!)&lt;/p&gt;
&lt;p&gt;Temporarily leveraging &lt;code&gt;pm.Potential&lt;/code&gt; to posit a likelihood, we design a truncated Dirichlet process (truncated because this is how we need to handle the infinite sum). Although a sample from a Dirichlet process is a &lt;strong&gt;discrete&lt;/strong&gt; distribution, we can evaluate its likelihood at the drawn weights and atoms:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathcal{L}(M; \pmb{w}, \pmb{m}, G_0) = \prod_{h=1}^K f_{G_0}(m_h) \text{Beta.pdf}(v_h)
\end{align*}&lt;/p&gt;
&lt;p&gt;where $f_{G_0}(\cdot)$ is the probability density or mass function of the base distribution $G_0$ and $v_h \stackrel{\small{\text{i.i.d.}}}{\sim} \text{Beta}(1, M)$ for some concentration parameter $M &amp;gt; 0$. However, given weights $w_h = v_h \prod_{\ell &amp;lt; h} (1 - v_\ell)$, we can recover the Beta random variables/realizations:&lt;/p&gt;
&lt;p&gt;\begin{align*}
v_h = \frac{w_h}{1 - \sum_{\ell &amp;lt; h} w_\ell} .
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;mistake-3-looking-to-closely-to-the-gaussian-process-submodule&#34;&gt;Mistake 3: Looking to closely to the Gaussian process submodule&lt;/h2&gt;
&lt;p&gt;Here is the 
&lt;a href=&#34;https://github.com/pymc-devs/pymc3/tree/main/pymc3/gp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;submodule&lt;/a&gt; and some 
&lt;a href=&#34;https://docs.pymc.io/Gaussian_Processes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;. Gaussian processes are similar to Dirichlet processes in that they are formally stochastic processes and allow users to posit little restrictions regarding the functional form of the estimand. From a practical perspective, Gaussian processes can be used as priors over arbitrary continuous functions whereas Dirichlet processes can serve as priors for distributions. However, when it comes to their implementation (especially in PyMC3/Aesara), the way how Gaussian processes deal with infinitum is different to the way how Dirichlet processes handles this notion. Broadly speaking, the Gaussian process submodule centers around providing &lt;code&gt;Xs&lt;/code&gt;, locations where we have observations, and providing inference for &lt;code&gt;Xnew&lt;/code&gt;. However, I initially thought about doing the same thing for Dirichlet processes in that, given observations &lt;code&gt;Xs&lt;/code&gt; from a distribution $G$ of interest, I can output inference on &amp;ldquo;atomic distributions&amp;rdquo; or, differently put, performing inference on $P(G \leq g | X)$ for selected values of $g$ (similarly to &lt;code&gt;Xnew&lt;/code&gt; for Gaussian processes).&lt;/p&gt;
&lt;h4 id=&#34;solution-to-mistake-3&#34;&gt;&lt;em&gt;Solution to Mistake 3&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Dirichlet processes can be represented by a specifically constructed infinite sum (see math notes below). However, in practice (i.e. in code), our computers &lt;strong&gt;cannot&lt;/strong&gt; sum up infinite terms hence we can truncate the sum at some large enough value $K$. That being said, a likelihood can be written out for a single sample from a Dirichlet process and, hopefully, have the base class of a &lt;code&gt;DirichletProcess&lt;/code&gt; inherent from &lt;code&gt;Distribution&lt;/code&gt; such that observed values can still be conditioned upon using an &lt;code&gt;obs=...&lt;/code&gt; argument (like all other distributions in PyMC3) (API design to be discussed in the coming weeks&amp;hellip;).&lt;/p&gt;
&lt;h2 id=&#34;mistake-4-diving-too-deep-into-theory&#34;&gt;Mistake 4: Diving too deep into theory&lt;/h2&gt;
&lt;p&gt;The mathematical properties of Dirichlet processes and how they work on paper is very important, don&amp;rsquo;t get me wrong. However, it is equally important to think about how these can translate into code and can be programmed using the PyMC3 lingo. There are many specifics about Dirichlet processes that I can effectively ignore without it affecting my learning process and implementation in code: what exactly is a random probability measure, convergence via the Bernstein-Von Mises theorem, reading original published papers decades ago, etc.&lt;/p&gt;
&lt;h4 id=&#34;solution-to-mistake-4&#34;&gt;&lt;em&gt;Solution to Mistake 4&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Gaging what&amp;rsquo;s enough of theory can be difficult, but my understanding is that here is short list what I need to know from the mathematical side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stick-breaking construction of Dirichlet process;&lt;/li&gt;
&lt;li&gt;Marginal and posterior distributions;&lt;/li&gt;
&lt;li&gt;Dirichlet process mixtures;&lt;/li&gt;
&lt;li&gt;How Figures 2.1, 2.2 and 2.3 are constructed in 
&lt;a href=&#34;https://www.springer.com/gp/book/9783319189673&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M√ºller et al. (2015)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, I can also look at the 
&lt;a href=&#34;https://cran.r-project.org/web/packages/dirichletprocess/dirichletprocess.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;dirichletprocess&lt;/code&gt;&lt;/a&gt; package in R. Not only is it well-documented, but it can also serve as a inspiration in constructing the PyMC3 submodule.&lt;/p&gt;
&lt;h2 id=&#34;mistake-5-running-in-circles-and-asking-the-wrong-questions&#34;&gt;Mistake 5: Running in circles and asking the wrong questions&lt;/h2&gt;
&lt;p&gt;When I would meet with my mentors, I would have a laundry list of questions, but I wouldn&amp;rsquo;t have anything to share (document, plots, agenda). Asking questions is good, but it would be more beneficial to organize something more systematic such that meetings can be more efficient and my ideas can be shared more clearly. When asking questions to the PyMC community, I tend to be already overwhelmed and my thoughts aren&amp;rsquo;t properly conveyed. For instance, a question that is often asked as a reply to me is: &amp;ldquo;Can you show an example?&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;solution-to-mistake-5&#34;&gt;&lt;em&gt;Solution to Mistake 5&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Although I have been recommended to ask once I spend beyond 20 minutes trying to debug something, it is equally or even more important to have some prepared that showcases my question via code and/or plots. For instance, here is a 
&lt;a href=&#34;https://github.com/larryshamalama/pymc3-playground/pull/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PR&lt;/a&gt; where I ask about shapes in building a stick-breaking Aesara &lt;code&gt;RandomVariable&lt;/code&gt; class. I also drafted a meeting 
&lt;a href=&#34;https://github.com/larryshamalama/pymc3-playground/blob/master/gsoc-meetings/july-15-2021.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;agenda&lt;/a&gt; in preparation for my meeting with my mentors, a 
&lt;a href=&#34;https://github.com/larryshamalama/pymc3-playground/blob/master/notebooks/dp-posterior.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt; on what I think is working well and other 
&lt;a href=&#34;https://github.com/larryshamalama/pymc3-playground/blob/master/notebooks/dp-mixture.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;
&lt;a href=&#34;https://github.com/larryshamalama/pymc3-playground/blob/master/notebooks/test-potential.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;s&lt;/a&gt; on what is not working as well as I would have liked. Moving forward, I intend to use these tools to better communicate my questions and show my ongoing progress.&lt;/p&gt;
&lt;h2 id=&#34;mistake-6-injuring-myself-on-sunday&#34;&gt;Mistake 6: Injuring myself on Sunday&amp;hellip;&lt;/h2&gt;
&lt;img src=&#34;slack.png&#34; width=&#34;75%&#34;&gt;
&lt;p&gt;turned into:&lt;/p&gt;
&lt;img src=&#34;skatepark-hospital.png&#34; width=&#34;75%&#34;&gt;
&lt;p&gt;from a trip to the skatepark&amp;hellip; Unsurprinsingly, I was not very productive later that evening nor the following day. Luckily, no bones were broken! üí™&lt;/p&gt;
&lt;h4 id=&#34;solution-to-mistake-6&#34;&gt;&lt;em&gt;Solution to Mistake 6&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;As prescribed by the doctor, rehabilitation at the sports, some ankle exercises and no more skating for the next few weeks (if not months)&amp;hellip; üòï&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Last but not least, imposter syndrome can feel very real and we all make progress at different rates. It is important to stay optimistic regarding my progress and my mentors (Austin and Chris) have been very supportive. Especially for a project and this opportunity that I care deeply about, I am motivated to keep learning, make even more mistakes and, hopefully one day, have a Dirichlet process submodule ready to be launched for public use.&lt;/p&gt;
&lt;h1 id=&#34;continuation-of-math-notes&#34;&gt;Continuation of Math Notes&lt;/h1&gt;
&lt;p&gt;As said above, diving too deep into the theory can lead to an endless recursion of readings. However, understanding what Dirichlet processes are is important!&lt;/p&gt;
&lt;h4 id=&#34;22-dirichlet-process&#34;&gt;2.2 Dirichlet Process&lt;/h4&gt;
&lt;p&gt;Temporary placeholder: Coming up! I need to study a bit for my upcoming exam first&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>git rebase, git commit, git teach me how to contribute to open source</title>
      <link>https://www.larrydong.com/gsoc2021/community-bonding-week-3-4/</link>
      <pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/community-bonding-week-3-4/</guid>
      <description>&lt;p&gt;The rest of community bonding was spent interacting with the PyMC developpers and, above all, learning how to better use GitHub. On the statistical theory side, things are progressing faster. Admittedly, on the coding side, I have come to full realization that my foundation is more lacking and there is a lot to learn.&lt;/p&gt;
&lt;p&gt;Emotionally, it has sometimes been difficult to accept my slow progress and feeling completely lost&amp;hellip; I feel that I am currently in a slump and am not too sure where to even look for directions. There are many weeks left, so lots of room to better myself!&lt;/p&gt;
&lt;p&gt;Here are some lessons and some areas that I need to work on for the coming weeks.&lt;/p&gt;
&lt;h4 id=&#34;1---managing-a-better-git-workflow&#34;&gt;1 - Managing a better Git workflow&lt;/h4&gt;
&lt;p&gt;I &lt;em&gt;will&lt;/em&gt; be working on different branches at a time. For instance, my implementation of a DP submodule should be done independently from addressing PyMC3 issues. Familiarizing myself with how to use &lt;code&gt;git merge&lt;/code&gt;, &lt;code&gt;git reset&lt;/code&gt; and &lt;code&gt;git rebase&lt;/code&gt; will be very important to ensure properly flow as as aspiring developper. For now, what I have gathered is summarized in the following steps as I am working on my branch &lt;code&gt;dp-gsoc&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;git commit&lt;/code&gt; my progress. I don&amp;rsquo;t have to push to &lt;code&gt;origin&lt;/code&gt; yet!&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git checkout main&lt;/code&gt; to go back to main and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git pull upstream&lt;/code&gt; to update my local repository with respect to &lt;code&gt;upstream&lt;/code&gt;. It would be also nice to push to &lt;code&gt;origin main&lt;/code&gt; with a simple &lt;code&gt;git push -u origin main&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git checkout dp-gsoc&lt;/code&gt; followed by&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git merge main&lt;/code&gt; to update my local branch with the updates from &lt;code&gt;upstream main&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is a failed-proof method since I am currently working on a brand new submodule, so I don&amp;rsquo;t expect to encounter any merge conflicts üòÖ&lt;/p&gt;
&lt;h4 id=&#34;2---asking-questions&#34;&gt;2 - Asking questions&lt;/h4&gt;
&lt;p&gt;Asking the right and right number of questions has also not been easy. I can spend countless hours asking PyMC developpers and contributors for pointers and answers, but, in many instances, I just often have a feeling that my questions are too &amp;ldquo;basic&amp;rdquo;. This &amp;ldquo;imposter syndrome&amp;rdquo; feeling is not fun, but I try very hard to go through all (if not most) of the documentation before asking any questions. However, if I&amp;rsquo;m stuck for any longer, I feel that I should not hesitate.&lt;/p&gt;
&lt;h4 id=&#34;3---knowing-what-i-dont-need-to-know&#34;&gt;3 - Knowing what I don&amp;rsquo;t need to know&lt;/h4&gt;
&lt;p&gt;When it came to understanding the codebase, it was initially difficult for me to discern what I &lt;em&gt;need&lt;/em&gt; to know for my project versus what I don&amp;rsquo;t need to know. For instance, PyMC3 is built on top 
&lt;a href=&#34;https://github.com/aesara-devs/aesara&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;aesara&lt;/a&gt; which, apparently, not everybody understands to its full extent. And that&amp;rsquo;s more than okay to keep working with my GSoC project :&#39;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üìö Diving into the math...</title>
      <link>https://www.larrydong.com/gsoc2021/community-bonding-week-2/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/community-bonding-week-2/</guid>
      <description>&lt;p&gt;My second week was spent mostly diving into textbooks to better understand what Dirichlet processes entail. To do so, I started with a brief revision of probability theory and reviewing Gaussian processes. Why? Given that I &lt;del&gt;am&lt;/del&gt; should be studying for my comprehensive exams, I thought that I better understand what it means to posit priors on the space of probability measures. As recommended by my mentors, a good starting point would be to look at Gaussian processes (GP) since I found Dirichlet processes very difficult to wrap my head around. GPs are convenient priors for continuous functions and they seem to be slightly more easy to understand.&lt;/p&gt;
&lt;p&gt;To document my learning process of statistical theory, I intend to summarize some key theoretical underpinnings in this blog. We&amp;rsquo;ll see where this goes! In light of all the math below, here one of my favorite Calvin and Hobbes strip.&lt;/p&gt;
&lt;img src=&#34;Calvin-and-Hobbes-Leave-Math-to-The-Machines.jpeg&#34; width=&#34;65%&#34;&gt;
&lt;p&gt;As a side note, I am part of the organizing committee for the 2021 Canadian Statistics Student Conference which will be happening this Saturday, June 5. Quebec also just announced that the administration of second vaccination doses have been sped up. All lot is going on, and it&amp;rsquo;s all exciting!&lt;/p&gt;
&lt;h2 id=&#34;1---probability-theory&#34;&gt;1 - Probability Theory&lt;/h2&gt;
&lt;p&gt;This section follows closely Chapters 1 - 3 from 
&lt;a href=&#34;http://static.stevereads.com/papers_to_read/probability_with_martingales_williams_.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probability with Martingales&lt;/a&gt;. For a more in-depth read, 
&lt;a href=&#34;https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probability and Measures&lt;/a&gt; sounds great but challenging. The main reason why I go through some (probably way too dense/deep) theory before jumping into Dirichlet processes (DP) is because DPs is a suitable prior over the space of probability measures.&lt;/p&gt;
&lt;h4 id=&#34;11---what-is-a-measure&#34;&gt;1.1 - What is a measure?&lt;/h4&gt;
&lt;p&gt;Formally, we start with $(S, \Sigma)$ be a measurable space where $S$ is some topological space and $\Sigma$ is the $\sigma$-algebra generated by open subsets of $S$. A mapping $\mu: \Sigma \rightarrow [0, +\infty]$ on $(S, \Sigma)$ is a &lt;em&gt;measure&lt;/em&gt; if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) $\mu(A) \geq 0$ for all $A \in \Sigma$ and;&lt;/li&gt;
&lt;li&gt;(2) $\mu$ is countably additive, i.e. $\mu\left(\cup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty \mu(A_i)$ and $\mu(\emptyset) = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transitioning from general, super abstract sets, $\sigma$-algebras and crazy measures, we usually define a probability triple $(\Omega, \mathcal{F}, \mathbb{P})$ as followed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$: sample space;&lt;/li&gt;
&lt;li&gt;$\mathcal{F}$: set of possible events $E \subseteq \Omega$ (and is a $\sigma$-algebra);&lt;/li&gt;
&lt;li&gt;$\mathbb{P}$: probability measure, i.e. a measure $\mu$ such that $\mu(\Omega) = 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another more intuitive way we can envision measurable spaces is as $(\Omega, \mathcal{F}) \stackrel{\text{in words}}{=}$ (sample space, family of events).&lt;/p&gt;
&lt;h4 id=&#34;12---what-is-a-random-variable&#34;&gt;1.2 - What is a random variable?&lt;/h4&gt;
&lt;p&gt;In statistics, we are interested in random variables and their properties. Random variables assign values to possible events in $\mathcal{F}$ on which probability functions $\mathbb{P}$ can be evaluated. Given a measurable space $(S, \Sigma)$, a class of functions $h: S \rightarrow \mathbb{R}$ is $\Sigma$-measurable if, for all $A \in \mathcal{B}$, its inverse $h^{-1}: \mathcal{B} \rightarrow \Sigma$ has the following property:&lt;/p&gt;
&lt;p&gt;\begin{align*}
h^{-1}(A) = \{s \in S \,\vert\,  h(s) \in A \}
\end{align*}&lt;/p&gt;
&lt;p&gt;where $\mathcal{B} := \sigma$(open sets of $\mathbb{R}$) is the Borel set.&lt;/p&gt;
&lt;p&gt;With this, a random variable is &amp;ldquo;simply&amp;rdquo; defined to be a mapping $X: \Omega \rightarrow \mathbb{R}$ that is $\Sigma$-measurable.&lt;/p&gt;
&lt;h4 id=&#34;13---summary-of-probability-theory-already&#34;&gt;1.3 - Summary of probability theory (already?)&lt;/h4&gt;
&lt;p&gt;The overarching idea is that, for a probability space (also called probability triple) $(\Omega, \mathcal{F}, \mathbb{P})$, we have the following mappings outlined:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\Omega &amp;amp;\stackrel{X}{\rightarrow} \mathbb{R}\\\&lt;br&gt;
\mathcal{B} \stackrel{X^{-1}}{\rightarrow} &amp;amp; \mathcal{F} \stackrel{\mathbb{P}}{\rightarrow} [0, 1] .
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;2---bayesian-nonparametrics&#34;&gt;2 - Bayesian nonparametrics&lt;/h2&gt;
&lt;p&gt;When performing inference, we often posit parametric assumptions on the data-generating mechanism. However, this can be restrictive especially when we don&amp;rsquo;t know the functional form of such underlying mechanism. In these situations, we can turn to nonparametric methods which, as the name suggests, do &lt;strong&gt;not&lt;/strong&gt; posit distributional assumptions and hence protects against model misspecification. In Bayesian nonparametrics (BNP), we can put priors on functions or probability distribution themselves. As Peter M√ºller once said when talking about ANOVA DDPs, a type of Dirichlet process: &amp;ldquo;A BNP is always right in the sense, no matter the true distribution, our prior always puts some probability mass in some neighborhood of the truth so we can learn about it. It has full support.&amp;rdquo; (see 
&lt;a href=&#34;https://youtu.be/qGZrkiLLhe4?t=2771&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt; here)&lt;/p&gt;
&lt;p&gt;Here are some references that I used during the week:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cambridge.org/core/books/bayesian-nonparametrics/884D64B861BEDA1CFA615B219F7086C5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Nonparametrics&lt;/a&gt; by N. Hjort et al. (2010);&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.springer.com/gp/book/9783319189673&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Nonparametric Data Analysis&lt;/a&gt; by P. M√ºller et al. (2015);&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cs.ubc.ca/~murphyk/MLbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning: a Probabilistic Perspective&lt;/a&gt; by K. Murphy (2012);&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;the probability theory textbooks mentioned above.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;21---gaussian-processes&#34;&gt;2.1 - Gaussian processes&lt;/h4&gt;
&lt;p&gt;By definition, a GP is a stochastic process in which any finite collection of observations $\mathcal{D} = \{(x_i, y_i = f(x_i) \}_{i=1}^N$ follows a multivariate normal (MVN) distribution with $N$-dimensional mean $\mu(\pmb{x})$ and a $N \times N$ covariance matrix $\kappa$ where the $i$, $j$th entry can be specified as followed:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\kappa_{ij} = \kappa(x_i, x_j) = \sigma^2_f \,\text{exp} \Big\{-\frac{1}{2\ell^2} (x_i - x_j)^2 \Big\}
\end{align*}&lt;/p&gt;
&lt;p&gt;for some $(\sigma^2_f, \ell^2) \in \mathbb{R}_+^2$. It is to my understanding that $\mu(\pmb{x})$ is often assumed to be $\vec{0}$ &lt;sub&gt;&lt;sup&gt;(for reasons that are unknown to me at the moment)&lt;/sup&gt;&lt;/sub&gt;.&lt;/p&gt;
&lt;p&gt;An interesting way of using GPs as prior distributions is that, say we have $N$ observations $(x_1, f(x_1)), \dots, (x_N, f(x_N))$, we want to perform inference on the unknown function $f$ &lt;em&gt;without&lt;/em&gt; any making any assumptions on its functional form. Because any collection of observations $\sim$ MVN, we are able to form predictions for $f(x_\star)$ for any unobserved $x_\star$. This stems from a well-established property of MVNs in that, if $(X_1, \dots, X_N) \sim \text{MVN}$, $\vec{\pmb{X}}_{\mathcal{I}_1} \, | \, \vec{\pmb{X}}_{\mathcal{I}_2} \sim \text{MVN}$ where $\mathcal{I}_1 \cup \mathcal{I}_2 = \{1, \dots, n\}$ and $\mathcal{I}_1 \cap \mathcal{I}_2 = \emptyset$. Using this, we have that:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\Big\{f(x_\star) \, | \, x_\star, \pmb{x}, f(\pmb{x})\Big\} &amp;amp;\sim \text{MVN}(\mu_\star, \Sigma_\star)\\\&lt;br&gt;
\text{where } \mu_\star &amp;amp;= \mu(\pmb{x}) + \kappa_\star^\text{T} \kappa^{-1}\big(f(\pmb{x}) - \mu(\pmb{x})\big)\\\&lt;br&gt;
\kappa_\star &amp;amp;= \kappa_{\star \star} - \kappa_{\star}^\text{T} \kappa^{-1} \kappa_{\star}
\end{align*}&lt;/p&gt;
&lt;p&gt;where $\kappa_{\star} = \kappa(\pmb{x}, x_\star)$ and $\kappa_{\star\star} = \kappa(x_\star, x_\star)$.&lt;/p&gt;
&lt;h6 id=&#34;brief-simulation&#34;&gt;Brief simulation&lt;/h6&gt;
&lt;p&gt;To visualize the results above, I wrote a short notebook that shows the 95% equi-tailed credible intervals for all $x$-values. Code is available 
&lt;a href=&#34;https://github.com/larryshamalama/pymc3-playground/blob/master/gaussian-processes.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
&lt;img src=&#34;gp-simulation.png&#34; width=&#34;75%&#34;&gt;
&lt;h4 id=&#34;22---dirichlet-processes&#34;&gt;2.2 - Dirichlet processes&lt;/h4&gt;
&lt;p&gt;Stay tuned, I hope to talk about DPs in my next GSoC blogpost!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üöÄ End of a Remote First Year PhD, Beginning of GSoC2021</title>
      <link>https://www.larrydong.com/gsoc2021/community-bonding-week-1/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/community-bonding-week-1/</guid>
      <description>&lt;p&gt;The first year of my PhD studies had, in all honesty, some rather unexpected challenges. Given the ongoing pandemic, my move to a new city for my PhD studies was put on hold. Instead, I decided to spend more quality time with one of my good friends (he&amp;rsquo;s a great cook!) and my parents. As spring was looming in, I quickly ran out of motivation to work on assignments and write my end-of-term report, but, with the help of my support network and all things considered, my first year ended well!&lt;/p&gt;
&lt;p&gt;With summer right around the corner, I was eager for some good news and an opportunity to look forward to. On Monday, May 17 right before 2pm, I received the following email and nearly floored with happiness:&lt;/p&gt;
&lt;img src=&#34;acceptance.png&#34; width=&#34;75%&#34;&gt;
&lt;p&gt;and I had to immediately share this amazing news with my parents and my friends! I am extremely grateful for the Google Summer of Code (GSoC) opportunity for the many reasons. Firstly, I will learn more about open source development and probabilistic programming from well-established mentors. Secondly, this practical opportunity would complement well my summer studying for my PhD comprehensive exams. Last but not least, I&amp;rsquo;m being paid! üòÖ&lt;/p&gt;
&lt;p&gt;Under the mentorship of 
&lt;a href=&#34;https://twitter.com/fonnesbeck&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christopher Fonnesbeck&lt;/a&gt; and 
&lt;a href=&#34;https://www.austinrochford.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Austin Rochford&lt;/a&gt;, my GSoC project centers around extending the 
&lt;a href=&#34;https://github.com/pymc-devs/pymc3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC3&lt;/a&gt; package with a Dirichlet process submodule. PyMC3 is a probabilistic framework in Python that allows users to fit Bayesian models via MCMC sampling. There is a growing interest in Bayesian nonparametric methods for fitting statistical models without specifying its parametric form. Dirichlet processes can be used as priors for probability distributions themselves. How exactly? I shall find out in the next three weeks of community bonding!&lt;/p&gt;
&lt;p&gt;In the coming months, I look forward to getting to know the PyMC3 community, reading more about Bayesian nonparametric statistics and, above all, learning from my mentors. Stay tuned for more!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üêç Conda Environments and Jupyter Notebooks</title>
      <link>https://www.larrydong.com/post/conda-cheatsheet/</link>
      <pubDate>Sat, 25 Apr 2020 15:04:34 -0400</pubDate>
      <guid>https://www.larrydong.com/post/conda-cheatsheet/</guid>
      <description>&lt;p&gt;Here is a cheatsheet in using Anaconda with some of my personal preferences that I have retained. If you do follow this, &lt;em&gt;please&lt;/em&gt; let me know if something is not working correctly!&lt;/p&gt;
&lt;img src=&#34;anaconda.png&#34; width=65%&gt;
&lt;h1 id=&#34;what-is-anaconda&#34;&gt;What is Anaconda?&lt;/h1&gt;
&lt;p&gt;Simply put, Anaconda is an open-source package management system for Python. One of its most popular applications, Jupyter, is commonly used for data science related projects. In this article, I have tried to detail the steps from creating a virtual environment in Anaconda to using it in your Jupyter notebooks.&lt;/p&gt;
&lt;h3 id=&#34;installing-anaconda&#34;&gt;Installing Anaconda&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Go to the Anaconda website 
&lt;a href=&#34;https://www.anaconda.com/products/individual&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Click on the appropriate distribution (Mac or Linux) and download the Anaconda installer for &lt;strong&gt;Python 3.7&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Open Terminal and execute the following commands:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cd ~/Downloads
chmod +x Anaconda3-2020.02-MacOSX-x86_64.sh
./Anaconda3-2020.02-MacOSX-x86_64.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;If you&amp;rsquo;re on a Linux distribution, the downloaded file has a different name, so execute the commands above with the downloaded file name.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;‚ùó You should reset your Terminal window by simply restarting it.&lt;/p&gt;
&lt;h1 id=&#34;setting-up-your-virtual-environment&#34;&gt;Setting up your virtual environment&lt;/h1&gt;
&lt;p&gt;Creating a virtual environment for your Python project has many benefits, including version control for you and your collaborators and managing sets of installed packages for separate projects. Here are 5 steps to creating your Anaconda virtual environment and using it in a Jupyter notebook.&lt;/p&gt;
&lt;h3 id=&#34;1---creating-the-virtual-environment&#34;&gt;1 - Creating the virtual environment&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;conda create -n YOUR_ENV_NAME python=3.7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command above specifies Python version in the environment to be 3.7, since this is the most stable and updated version at the moment. However, you can use a different version if you prefer.&lt;/p&gt;
&lt;h3 id=&#34;2---creating-a-terminal-shortcut&#34;&gt;2 - Creating a Terminal Shortcut&lt;/h3&gt;
&lt;p&gt;My preference is to create a Terminal shortcut to activate the environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;echo &amp;quot;alias SHORTCUT_COMMAND=&#39;conda activate YOUR_ENV_NAME&#39;&amp;quot; | cat - ~/.bash_profile &amp;gt; ~/.bash_profile
source ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;If you are on a Linux distribution, replace &lt;code&gt;~/.bash_profile&lt;/code&gt; by &lt;code&gt;~/.bashrc&lt;/code&gt; due to the difference in file names.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;3---activating-your-environment&#34;&gt;3 - Activating your environment&lt;/h3&gt;
&lt;p&gt;With the shortcut created using the commands above, you can now activate the environment by simply typing its name in Terminal!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;YOUR_ENV_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;4---installing-packages&#34;&gt;4 - Installing packages&lt;/h3&gt;
&lt;p&gt;Say you want to install some Python packages called &lt;code&gt;package1&lt;/code&gt;, &lt;code&gt;package2&lt;/code&gt; and &lt;code&gt;package3&lt;/code&gt;. You can use the code below to do so.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -n YOUR_ENV_NAME package1 package2 package3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5---adding-the-environment-to-jupyter&#34;&gt;5 - Adding the environment to Jupyter&lt;/h3&gt;
&lt;p&gt;Often times, it would be desirable to be able to access the environment in a Jupyter notebook, which is often used for coding in Python (it&amp;rsquo;s definitely my favorite!). To add your newly created environment to Jupyter, run the following commands in Terminal.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -n YOUR_ENV_NAME ipykernel -c conda-forge
python -m ipykernel install --user --name YOUR_ENV_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;running-into-command-not-found-jupyter&#34;&gt;Running into &lt;code&gt;command not found: jupyter&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;One of the packages that you may have to install is jupyter itself!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -n YOUR_ENV_NAME -c conda-forge jupyterlab
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;ready-to-code&#34;&gt;Ready to code!&lt;/h1&gt;
&lt;p&gt;Your environment should be ready to use in your Jupyter notebooks!&lt;/p&gt;
&lt;/br&gt;
&lt;h1 id=&#34;other-hopefully-useful-things&#34;&gt;Other (hopefully) useful things&lt;/h1&gt;
&lt;h3 id=&#34;locating-your-virtual-environment&#34;&gt;Locating your virtual environment&lt;/h3&gt;
&lt;p&gt;My preference is to go the folder in which your environment is installed and simply removing it. To find the directory in which your virtual environment is located, you can do:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source activate YOUR_ENV_NAME # to activate your environment
which python
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should get an output under the following format: &lt;code&gt;SOME_DIRECTORY/YOUR_ENV_NAME/bin/python&lt;/code&gt;. The directory in which your environment is located is simply &lt;code&gt;SOME_DIRECTORY&lt;/code&gt;.&lt;/p&gt;
&lt;h5 id=&#34;example&#34;&gt;Example&lt;/h5&gt;
&lt;p&gt;When I run &lt;code&gt;which python&lt;/code&gt; after activating my environment called &lt;code&gt;ml&lt;/code&gt;, I obtain &lt;code&gt;/Users/larry/anaconda3/envs/ml/bin/python&lt;/code&gt;. It follows that the directory where my conda environments are located is &lt;code&gt;Users/larry/anaconda3/envs/&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;deleting-your-environment&#34;&gt;Deleting your environment&lt;/h3&gt;
&lt;p&gt;Once you have located your environment (see above) and found the directory in which your environments are located and that we will call &lt;code&gt;SOME_DIRECTORY&lt;/code&gt;. To delete your environment, run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd SOME_DIRECTORY
rm -rf YOUR_ENV_NAME
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;removing-your-environment-from-jupyter&#34;&gt;Removing your environment from Jupyter&lt;/h3&gt;
&lt;p&gt;Although you have deleted your environment and it no longer exists, for some reason the option to select it as a kernel is still available in your Jupyter notebook, although selecting it won&amp;rsquo;t work. To remove the environment as an option in the Jupyter notebook, run the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter kernelspec uninstall YOUR_ENV_NAME
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
