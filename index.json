[{"authors":["admin"],"categories":null,"content":"I am a first year PhD student in biostatistics at the Dalla Lana School of Public Health at the University of Toronto. My research interests include causal inference, Bayesian methods and their application to observational data. I am also broadly interested in developing inference-based statistical models for matrix-valued or imaging data.\nFollowing my remote first year of PhD studies, I am joining PyMC3 as a Google Summer of Code student! You can follow my progress in my blog here.\nPreviously, I have pursued a dual Master\u0026rsquo;s degree under an IVADO scholarship: one in Biostatistics at McGill University and one in Public Health Data Science at the University of Bordeaux. I was jointly supervised by Dr. Erica Moodie and Dr. Rodolphe Thi√©baut.\nI also enjoy coding in Python, skateboarding and eating yogurt. Feel free to connect with me!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1621468711,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.larrydong.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a first year PhD student in biostatistics at the Dalla Lana School of Public Health at the University of Toronto. My research interests include causal inference, Bayesian methods and their application to observational data.","tags":null,"title":"Larry Dong","type":"authors"},{"authors":[],"categories":[],"content":"With half of GSoC behind me (already!), progress in the past few weeks have been embarrassingly slow. The first challenge is learning how to learn. As I wrote in my previous post, there is so much that I don\u0026rsquo;t know; while this is inherently one of the exciting aspects of GSoC, it can be daunting to run around in circles. From the testing in the CD/CI pipeline to writing a likelihood for a sample from a Dirichlet Process, the first 5 weeks of GSoC have been invested into making mistakes. As my mentors have said in the first evaluation: \u0026ldquo;We encourage him to continue to focus on quantity rather than quality, and don\u0026rsquo;t be afraid to make mistakes \u0026ndash; a ton of learning occurs from iterating on code.\u0026rdquo;\nAlthough I have little code to actually talk about, here is a rundown of \u0026ldquo;mistakes\u0026rdquo; that I learned during the past weeks.\nMistake 1: Attempting to get too involved with version 4 refactoring of PyMC3 PyMC3 is currently undergoing a refactoring process converting its backend structure to Aesara which, frankly, is difficult to wrap my head around. My attempt to refactor the $\\chi^2$ distribution (see PR here and here) was an \u0026ldquo;easy\u0026rdquo; exercise that took a long time to figure out during the (pre-)community bonding phase. Dirichlet processes are often used to model the latent parameter in mixture models such that the number of clusters need not to be specified. However, with pm.Mixture and pm.DensityDist needing refactoring, I initially wanted to challenge myself to help write an rv_op for mixture models, but it turns out to be quite challenging (see PR here).\nSolution to Mistake 1 Rather than spend time figuring out how to rewrite a mixture rv_op, it is more worthwhile to spend my time implementing a single class for Dirichlet processes (mixtures) and leverage pm.Mixture once it has been refactored. Dirichlet processes are complicated on its own, so it is okay that I solely focus on them.\nMistake 2: Coding Dirichlet processes without a likelihood PyMC3 relies on using pm.Model() as a context manager such that distributions can be posited on random variables and parameters. Given that a Dirichlet process $G$ can be rewritten as:\n\\begin{align*} G(x) = \\sum_{h=1}^{\\infty} w_h \\delta_{m_h}(x) \\end{align*}\nfor sticks $w_h$ and atoms $m_h$ (see below for more mathematical details), this is exactly what I coded using Aesara operations, which are very similar to NumPy ones.\nimport pymc3 as pm import numpy as np import aesara.tensor as at def stick_breaking(betas): sticks = at.concatenate([[1], at.cumprod(1 - betas[:-1])]) return betas * sticks Xs = np.array([-1, 0, 0.5, 1, 2])[..., np.newaxis] K = 19 # number of sticks with pm.Model() as model: betas = pm.Beta(\u0026quot;betas\u0026quot;, 1., np.ones(shape=(K,))) weights = pm.Deterministic(\u0026quot;weights\u0026quot;, stick_breaking(betas)) G0 = pm.Normal(\u0026quot;base-dist\u0026quot;, 0, 1) dirac = at.lt(G0, Xs) linear_comb = at.sum(at.mul(weights, dirac), axis=1) prior_atoms = pm.Deterministic( name=\u0026quot;dp-prior\u0026quot;, var=linear_comb, )  It worked, but the issue is that pm.Deterministic does not have a likelihood of its own. PyMC3 revolves around building a likelihood behind the scenes on which Hamiltonian Monte Carlo sampling methods can be executed. While the code above works, it is not the right direction when it comes to building an API and integrating it in more complex and potentially hierarchical models.\nSolution to Mistake 2 The solution to this is \u0026ldquo;simply\u0026rdquo; to build a class that implements a likelihood for a Dirichlet process. However, how do we write a likelihood for a sample from a Dirichlet process? Do nonparametric methods, frequentist or Bayesian, have likelihoods? (The answer is yes!)\nTemporarily leveraging pm.Potential to posit a likelihood, we design a truncated Dirichlet process (truncated because this is how we need to handle the infinite sum). Although a sample from a Dirichlet process is a discrete distribution, we can evaluate its likelihood at the drawn weights and atoms:\n\\begin{align*} \\mathcal{L}(M; \\pmb{w}, \\pmb{m}, G_0) = \\prod_{h=1}^K f_{G_0}(m_h) \\text{Beta.pdf}(v_h) \\end{align*}\nwhere $f_{G_0}(\\cdot)$ is the probability density or mass function of the base distribution $G_0$ and $v_h \\stackrel{\\small{\\text{i.i.d.}}}{\\sim} \\text{Beta}(1, M)$ for some concentration parameter $M \u0026gt; 0$. However, given weights $w_h = v_h \\prod_{\\ell \u0026lt; h} (1 - v_\\ell)$, we can recover the Beta random variables/realizations:\n\\begin{align*} v_h = \\frac{w_h}{1 - \\sum_{\\ell \u0026lt; h} w_\\ell} . \\end{align*}\nMistake 3: Diving too deep into theory The mathematical properties of Dirichlet processes and how they work on paper is very important, don\u0026rsquo;t get me wrong. However, it is equally important to think about how these can translate into code and can be programmed using the PyMC3 lingo. There are many specifics about Dirichlet processes that I can effectively ignore without it affecting my learning process and implementation in code: what exactly is a random probability measure, convergence via the Bernstein-Von Mises theorem, reading original published papers decades ago, etc.\nSolution to Mistake 3 Gaging what\u0026rsquo;s enough of theory can be difficult, but my understanding is that here is short list what I need to know from the mathematical side:\n Stick-breaking construction of Dirichlet process; Marginal and posterior distributions; Dirichlet process mixtures; How Figures 2.1, 2.2 and 2.3 are constructed in M√ºller et al. (2015).  Also, I can also look at the dirichletprocess package in R. Not only is it well-documented, but it can also serve as a inspiration in constructing the PyMC3 submodule.\nMistake 4: Running in circles and asking the wrong questions When I would meet with my mentors, I would have a laundry list of questions, but I wouldn\u0026rsquo;t have anything to share (document, plots, agenda). Asking questions is good, but it would be more beneficial to organize something more systematic such that meetings can be more efficient and my ideas can be shared more clearly. When asking questions to the PyMC community, I tend to be already overwhelmed and my thoughts aren\u0026rsquo;t properly conveyed. For instance, a question that is often asked as a reply to me is: \u0026ldquo;Can you show an example?\u0026rdquo;\nSolution to Mistake 4 Although I have been recommended to ask once I spend beyond 20 minutes trying to debug something, it is equally or even more important to have some prepared that showcases my question via code and/or plots. For instance, here is a PR where I ask about shapes in building a stick-breaking Aesara RandomVariable class. I also drafted a meeting agenda in preparation for my meeting with my mentors, a notebook on what I think is working well and other notebook s on what is not working as well as I would have liked.\nContinuation of Math Notes As said above, diving too deep into the theory can lead to an endless recursion of readings. However, understanding what Dirichlet processes are is important!\n2.2 Dirichlet Process Temporary placeholder: Coming up! I need to study a bit for my upcoming exam first\u0026hellip;\n","date":1626480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626480000,"objectID":"2ecd6ab6999d28a0b9f918f811972dbd","permalink":"https://www.larrydong.com/gsoc2021/gsoc-part1/","publishdate":"2021-07-17T00:00:00Z","relpermalink":"/gsoc2021/gsoc-part1/","section":"gsoc2021","summary":"'Quantity over quality' is not a tip that I often receive","tags":[],"title":"üê¢ The slump...","type":"gsoc2021"},{"authors":[],"categories":[],"content":"The rest of community bonding was spent interacting with the PyMC developpers and, above all, learning how to better use GitHub. On the statistical theory side, things are progressing faster. Admittedly, on the coding side, I have come to full realization that my foundation is more lacking and there is a lot to learn.\nEmotionally, it has sometimes been difficult to accept my slow progress and feeling completely lost\u0026hellip; I feel that I am currently in a slump and am not too sure where to even look for directions. There are many weeks left, so lots of room to better myself!\nHere are some lessons and some areas that I need to work on for the coming weeks.\n1 - Managing a better Git workflow I will be working on different branches at a time. For instance, my implementation of a DP submodule should be done independently from addressing PyMC3 issues. Familiarizing myself with how to use git merge, git reset and git rebase will be very important to ensure properly flow as as aspiring developper. For now, what I have gathered is summarized in the following steps as I am working on my branch dp-gsoc:\n git commit my progress. I don\u0026rsquo;t have to push to origin yet! git checkout main to go back to main and git pull upstream to update my local repository with respect to upstream. It would be also nice to push to origin main with a simple git push -u origin main. git checkout dp-gsoc followed by git merge main to update my local branch with the updates from upstream main  This is a failed-proof method since I am currently working on a brand new submodule, so I don\u0026rsquo;t expect to encounter any merge conflicts üòÖ\n2 - Asking questions Asking the right and right number of questions has also not been easy. I can spend countless hours asking PyMC developpers and contributors for pointers and answers, but, in many instances, I just often have a feeling that my questions are too \u0026ldquo;basic\u0026rdquo;. This \u0026ldquo;imposter syndrome\u0026rdquo; feeling is not fun, but I try very hard to go through all (if not most) of the documentation before asking any questions. However, if I\u0026rsquo;m stuck for any longer, I feel that I should not hesitate.\n3 - Knowing what I don\u0026rsquo;t need to know When it came to understanding the codebase, it was initially difficult for me to discern what I need to know for my project versus what I don\u0026rsquo;t need to know. For instance, PyMC3 is built on top aesara which, apparently, not everybody understands to its full extent. And that\u0026rsquo;s more than okay to keep working with my GSoC project :')\n","date":1622678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622678400,"objectID":"e604a756297006f3c60ed0789042e529","permalink":"https://www.larrydong.com/gsoc2021/community-bonding-week-3-4/","publishdate":"2021-06-03T00:00:00Z","relpermalink":"/gsoc2021/community-bonding-week-3-4/","section":"gsoc2021","summary":"One line of code per headscratch per hour staring at my IDE","tags":[],"title":"git rebase, git commit, git teach me how to contribute to open source","type":"gsoc2021"},{"authors":[],"categories":[],"content":"My second week was spent mostly diving into textbooks to better understand what Dirichlet processes entail. To do so, I started with a brief revision of probability theory and reviewing Gaussian processes. Why? Given that I am should be studying for my comprehensive exams, I thought that I better understand what it means to posit priors on the space of probability measures. As recommended by my mentors, a good starting point would be to look at Gaussian processes (GP) since I found Dirichlet processes very difficult to wrap my head around. GPs are convenient priors for continuous functions and they seem to be slightly more easy to understand.\nTo document my learning process of statistical theory, I intend to summarize some key theoretical underpinnings in this blog. We\u0026rsquo;ll see where this goes! In light of all the math below, here one of my favorite Calvin and Hobbes strip.\nAs a side note, I am part of the organizing committee for the 2021 Canadian Statistics Student Conference which will be happening this Saturday, June 5. Quebec also just announced that the administration of second vaccination doses have been sped up. All lot is going on, and it\u0026rsquo;s all exciting!\n1 - Probability Theory This section follows closely Chapters 1 - 3 from Probability with Martingales. For a more in-depth read, Probability and Measures sounds great but challenging. The main reason why I go through some (probably way too dense/deep) theory before jumping into Dirichlet processes (DP) is because DPs is a suitable prior over the space of probability measures.\n1.1 - What is a measure? Formally, we start with $(S, \\Sigma)$ be a measurable space where $S$ is some topological space and $\\Sigma$ is the $\\sigma$-algebra generated by open subsets of $S$. A mapping $\\mu: \\Sigma \\rightarrow [0, +\\infty]$ on $(S, \\Sigma)$ is a measure if:\n (1) $\\mu(A) \\geq 0$ for all $A \\in \\Sigma$ and; (2) $\\mu$ is countably additive, i.e. $\\mu\\left(\\cup_{i=1}^\\infty A_i\\right) = \\sum_{i=1}^\\infty \\mu(A_i)$ and $\\mu(\\emptyset) = 0$.  Transitioning from general, super abstract sets, $\\sigma$-algebras and crazy measures, we usually define a probability triple $(\\Omega, \\mathcal{F}, \\mathbb{P})$ as followed:\n $\\Omega$: sample space; $\\mathcal{F}$: set of possible events $E \\subseteq \\Omega$ (and is a $\\sigma$-algebra); $\\mathbb{P}$: probability measure, i.e. a measure $\\mu$ such that $\\mu(\\Omega) = 1$.  Another more intuitive way we can envision measurable spaces is as $(\\Omega, \\mathcal{F}) \\stackrel{\\text{in words}}{=}$ (sample space, family of events).\n1.2 - What is a random variable? In statistics, we are interested in random variables and their properties. Random variables assign values to possible events in $\\mathcal{F}$ on which probability functions $\\mathbb{P}$ can be evaluated. Given a measurable space $(S, \\Sigma)$, a class of functions $h: S \\rightarrow \\mathbb{R}$ is $\\Sigma$-measurable if, for all $A \\in \\mathcal{B}$, its inverse $h^{-1}: \\mathcal{B} \\rightarrow \\Sigma$ has the following property:\n\\begin{align*} h^{-1}(A) = \\{s \\in S \\,\\vert\\, h(s) \\in A \\} \\end{align*}\nwhere $\\mathcal{B} := \\sigma$(open sets of $\\mathbb{R}$) is the Borel set.\nWith this, a random variable is \u0026ldquo;simply\u0026rdquo; defined to be a mapping $X: \\Omega \\rightarrow \\mathbb{R}$ that is $\\Sigma$-measurable.\n1.3 - Summary of probability theory (already?) The overarching idea is that, for a probability space (also called probability triple) $(\\Omega, \\mathcal{F}, \\mathbb{P})$, we have the following mappings outlined:\n\\begin{align*} \\Omega \u0026amp;\\stackrel{X}{\\rightarrow} \\mathbb{R}\\\\\\\n\\mathcal{B} \\stackrel{X^{-1}}{\\rightarrow} \u0026amp; \\mathcal{F} \\stackrel{\\mathbb{P}}{\\rightarrow} [0, 1] . \\end{align*}\n2 - Bayesian nonparametrics When performing inference, we often posit parametric assumptions on the data-generating mechanism. However, this can be restrictive especially when we don\u0026rsquo;t know the functional form of such underlying mechanism. In these situations, we can turn to nonparametric methods which, as the name suggests, do not posit distributional assumptions and hence protects against model misspecification. In Bayesian nonparametrics (BNP), we can put priors on functions or probability distribution themselves. As Peter M√ºller once said when talking about ANOVA DDPs, a type of Dirichlet process: \u0026ldquo;A BNP is always right in the sense, no matter the true distribution, our prior always puts some probability mass in some neighborhood of the truth so we can learn about it. It has full support.\u0026rdquo; (see video here)\nHere are some references that I used during the week:\n  Bayesian Nonparametrics by N. Hjort et al. (2010);  Bayesian Nonparametric Data Analysis by P. M√ºller et al. (2015);  Machine Learning: a Probabilistic Perspective by K. Murphy (2012);   the probability theory textbooks mentioned above.    2.1 - Gaussian processes By definition, a GP is a stochastic process in which any finite collection of observations $\\mathcal{D} = \\{(x_i, y_i = f(x_i) \\}_{i=1}^N$ follows a multivariate normal (MVN) distribution with $N$-dimensional mean $\\mu(\\pmb{x})$ and a $N \\times N$ covariance matrix $\\kappa$ where the $i$, $j$th entry can be specified as followed:\n\\begin{align*} \\kappa_{ij} = \\kappa(x_i, x_j) = \\sigma^2_f \\,\\text{exp} \\Big\\{-\\frac{1}{2\\ell^2} (x_i - x_j)^2 \\Big\\} \\end{align*}\nfor some $(\\sigma^2_f, \\ell^2) \\in \\mathbb{R}_+^2$. It is to my understanding that $\\mu(\\pmb{x})$ is often assumed to be $\\vec{0}$ (for reasons that are unknown to me at the moment).\nAn interesting way of using GPs as prior distributions is that, say we have $N$ observations $(x_1, f(x_1)), \\dots, (x_N, f(x_N))$, we want to perform inference on the unknown function $f$ without any making any assumptions on its functional form. Because any collection of observations $\\sim$ MVN, we are able to form predictions for $f(x_\\star)$ for any unobserved $x_\\star$. This stems from a well-established property of MVNs in that, if $(X_1, \\dots, X_N) \\sim \\text{MVN}$, $\\vec{\\pmb{X}}_{\\mathcal{I}_1} \\, | \\, \\vec{\\pmb{X}}_{\\mathcal{I}_2} \\sim \\text{MVN}$ where $\\mathcal{I}_1 \\cup \\mathcal{I}_2 = \\{1, \\dots, n\\}$ and $\\mathcal{I}_1 \\cap \\mathcal{I}_2 = \\emptyset$. Using this, we have that:\n\\begin{align*} \\Big\\{f(x_\\star) \\, | \\, x_\\star, \\pmb{x}, f(\\pmb{x})\\Big\\} \u0026amp;\\sim \\text{MVN}(\\mu_\\star, \\Sigma_\\star)\\\\\\\n\\text{where } \\mu_\\star \u0026amp;= \\mu(\\pmb{x}) + \\kappa_\\star^\\text{T} \\kappa^{-1}\\big(f(\\pmb{x}) - \\mu(\\pmb{x})\\big)\\\\\\\n\\kappa_\\star \u0026amp;= \\kappa_{\\star \\star} - \\kappa_{\\star}^\\text{T} \\kappa^{-1} \\kappa_{\\star} \\end{align*}\nwhere $\\kappa_{\\star} = \\kappa(\\pmb{x}, x_\\star)$ and $\\kappa_{\\star\\star} = \\kappa(x_\\star, x_\\star)$.\nBrief simulation To visualize the results above, I wrote a short notebook that shows the 95% equi-tailed credible intervals for all $x$-values. Code is available here!\n2.2 - Dirichlet processes Stay tuned, I hope to talk about DPs in my next GSoC blogpost!\n","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"56d69d80e767139ce6fa9887a3c8ff3f","permalink":"https://www.larrydong.com/gsoc2021/community-bonding-week-2/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/gsoc2021/community-bonding-week-2/","section":"gsoc2021","summary":"Math is... pretty difficult.","tags":[],"title":"üìö Diving into the math...","type":"gsoc2021"},{"authors":[],"categories":[],"content":"The first year of my PhD studies had, in all honesty, some rather unexpected challenges. Given the ongoing pandemic, my move to a new city for my PhD studies was put on hold. Instead, I decided to spend more quality time with one of my good friends (he\u0026rsquo;s a great cook!) and my parents. As spring was looming in, I quickly ran out of motivation to work on assignments and write my end-of-term report, but, with the help of my support network and all things considered, my first year ended well!\nWith summer right around the corner, I was eager for some good news and an opportunity to look forward to. On Monday, May 17 right before 2pm, I received the following email and nearly floored with happiness:\nand I had to immediately share this amazing news with my parents and my friends! I am extremely grateful for the Google Summer of Code (GSoC) opportunity for the many reasons. Firstly, I will learn more about open source development and probabilistic programming from well-established mentors. Secondly, this practical opportunity would complement well my summer studying for my PhD comprehensive exams. Last but not least, I\u0026rsquo;m being paid! üòÖ\nUnder the mentorship of Christopher Fonnesbeck and Austin Rochford, my GSoC project centers around extending the PyMC3 package with a Dirichlet process submodule. PyMC3 is a probabilistic framework in Python that allows users to fit Bayesian models via MCMC sampling. There is a growing interest in Bayesian nonparametric methods for fitting statistical models without specifying its parametric form. Dirichlet processes can be used as priors for probability distributions themselves. How exactly? I shall find out in the next three weeks of community bonding!\nIn the coming months, I look forward to getting to know the PyMC3 community, reading more about Bayesian nonparametric statistics and, above all, learning from my mentors. Stay tuned for more!\n","date":1621468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621468800,"objectID":"6e3f5f62360ef1edfbbaf47a614553da","permalink":"https://www.larrydong.com/gsoc2021/community-bonding-week-1/","publishdate":"2021-05-20T00:00:00Z","relpermalink":"/gsoc2021/community-bonding-week-1/","section":"gsoc2021","summary":"With my first year of PhD studies coming to an end, what better way to celebrate the beginning of a second summer during a pandemic than studying for my comprehensive exams participating in Google Summer of Code!","tags":[],"title":"üöÄ End of a Remote First Year PhD, Beginning of GSoC2021","type":"gsoc2021"},{"authors":[],"categories":[],"content":"Here is a cheatsheet in using Anaconda with some of my personal preferences that I have retained. If you do follow this, please let me know if something is not working correctly!\nWhat is Anaconda? Simply put, Anaconda is an open-source package management system for Python. One of its most popular applications, Jupyter, is commonly used for data science related projects. In this article, I have tried to detail the steps from creating a virtual environment in Anaconda to using it in your Jupyter notebooks.\nInstalling Anaconda  Go to the Anaconda website here. Click on the appropriate distribution (Mac or Linux) and download the Anaconda installer for Python 3.7. Open Terminal and execute the following commands:  cd ~/Downloads chmod +x Anaconda3-2020.02-MacOSX-x86_64.sh ./Anaconda3-2020.02-MacOSX-x86_64.sh  If you\u0026rsquo;re on a Linux distribution, the downloaded file has a different name, so execute the commands above with the downloaded file name.\n‚ùó You should reset your Terminal window by simply restarting it.\nSetting up your virtual environment Creating a virtual environment for your Python project has many benefits, including version control for you and your collaborators and managing sets of installed packages for separate projects. Here are 5 steps to creating your Anaconda virtual environment and using it in a Jupyter notebook.\n1 - Creating the virtual environment conda create -n YOUR_ENV_NAME python=3.7  The command above specifies Python version in the environment to be 3.7, since this is the most stable and updated version at the moment. However, you can use a different version if you prefer.\n2 - Creating a Terminal Shortcut My preference is to create a Terminal shortcut to activate the environment.\necho \u0026quot;alias SHORTCUT_COMMAND='conda activate YOUR_ENV_NAME'\u0026quot; | cat - ~/.bash_profile \u0026gt; ~/.bash_profile source ~/.bash_profile  If you are on a Linux distribution, replace ~/.bash_profile by ~/.bashrc due to the difference in file names.\n3 - Activating your environment With the shortcut created using the commands above, you can now activate the environment by simply typing its name in Terminal!\nYOUR_ENV_NAME  4 - Installing packages Say you want to install some Python packages called package1, package2 and package3. You can use the code below to do so.\nconda install -n YOUR_ENV_NAME package1 package2 package3  5 - Adding the environment to Jupyter Often times, it would be desirable to be able to access the environment in a Jupyter notebook, which is often used for coding in Python (it\u0026rsquo;s definitely my favorite!). To add your newly created environment to Jupyter, run the following commands in Terminal.\nconda install -n YOUR_ENV_NAME ipykernel -c conda-forge python -m ipykernel install --user --name YOUR_ENV_NAME  Running into command not found: jupyter One of the packages that you may have to install is jupyter itself!\nconda install -n YOUR_ENV_NAME -c conda-forge jupyterlab  Ready to code! Your environment should be ready to use in your Jupyter notebooks!\n Other (hopefully) useful things Locating your virtual environment My preference is to go the folder in which your environment is installed and simply removing it. To find the directory in which your virtual environment is located, you can do:\nsource activate YOUR_ENV_NAME # to activate your environment which python  You should get an output under the following format: SOME_DIRECTORY/YOUR_ENV_NAME/bin/python. The directory in which your environment is located is simply SOME_DIRECTORY.\nExample When I run which python after activating my environment called ml, I obtain /Users/larry/anaconda3/envs/ml/bin/python. It follows that the directory where my conda environments are located is Users/larry/anaconda3/envs/.\nDeleting your environment Once you have located your environment (see above) and found the directory in which your environments are located and that we will call SOME_DIRECTORY. To delete your environment, run the following commands:\ncd SOME_DIRECTORY rm -rf YOUR_ENV_NAME  Removing your environment from Jupyter Although you have deleted your environment and it no longer exists, for some reason the option to select it as a kernel is still available in your Jupyter notebook, although selecting it won\u0026rsquo;t work. To remove the environment as an option in the Jupyter notebook, run the following.\njupyter kernelspec uninstall YOUR_ENV_NAME  ","date":1587841474,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621468711,"objectID":"1c7ccf0e695f579f2603b85c990035cf","permalink":"https://www.larrydong.com/post/conda-cheatsheet/","publishdate":"2020-04-25T15:04:34-04:00","relpermalink":"/post/conda-cheatsheet/","section":"post","summary":"Here is a cheatsheet in using Anaconda with some of my personal preferences that I have retained. If you do follow this, please let me know if something is not working correctly!","tags":[],"title":"üêç Conda Environments and Jupyter Notebooks","type":"post"}]